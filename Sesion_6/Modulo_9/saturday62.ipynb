{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom fastai.basics import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\nEn esta parte de la lectura, explicamos Stochastic Gradient Descent (SGD) el cual es un metodo de **optimization** ampliamente utilizado en redes neuronales. Los siguientes son ejemplos ilustrativos.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#  Regresion Lineal"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "El objetivo es ajustar una linea recta a un set de puntos."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n=100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = torch.ones(n,2) \nx[:,0].uniform_(-1.,1)\nx[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = tensor(3.,2); a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y = x@a + torch.rand(n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(x[:,0], y);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Se desea encontrar **parametros** (pesos) `a` tal que minimicen el *error* entre los puntos y la linea `x@a`. Note que  `a` es desconocido. Para un problema de regreasion la *funcion de error* o *funcion de perdida* es la **media cuadrada del error**. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def mse(y_hat, y): return ((y_hat-y)**2).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Suponga que `a = (-1.0,1.0)` entonces podemos calcular `y_hat` la *prediction* y calcular el error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = tensor(-1.,1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_hat = x@a\nmse(y_hat, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],y_hat);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ya esta especificado el *modelo* (regresion lineal) y el *criterio de evaluacion* (o *funcion de perdida*). Ahora necesitamos manejar la  *optimizacion*; esto es, como encontrar el mejor valor para `a`? Como encontramos el mejor *ajuste* de la regresion lineal."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Decendente del Gradiente"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Queremos encontrar valores de `a` que minimicen `mse_loss`.\n\n**Decendente del Gradiente** Es un algoritmo que minimiza funciones. dada una función definida por un set de parametros , Decendente del Gradiente comienza con un set inicial de valores e iterativamente avanza hacia el set de parametros que minimizan la función. Esta minimización es alcanzada en direccion opuesta del gradiente."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = nn.Parameter(a); a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def update():\n    y_hat = x@a\n    loss = mse(y, y_hat)\n    if t % 10 == 0: print(loss)\n    loss.backward()\n    with torch.no_grad():\n        a.sub_(lr * a.grad)\n        a.grad.zero_()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lr = 1e-1\nfor t in range(100): update()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],x@a);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Animado!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from matplotlib import animation, rc\nrc('animation', html='jshtml')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = nn.Parameter(tensor(-1.,1))\n\nfig = plt.figure()\nplt.scatter(x[:,0], y, c='orange')\nline, = plt.plot(x[:,0], x@a)\nplt.close()\n\ndef animate(i):\n    update()\n    line.set_ydata(x@a)\n    return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Terminología"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "- Learning rate\n- Epoch\n- Minibatch\n- SGD\n- Model / Architecture\n- Parameters\n- Loss function\n\nPara problemas de clasificación, utilizamos *cross entropy loss*,conocida como *negative log likelihood loss*. ."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.5",
      "language": "python"
    },
    "language_info": {
      "version": "3.5.5",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}