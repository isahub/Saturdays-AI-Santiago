{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduci√≥n a APIs\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install tweepy\nimport tweepy\nimport pandas as pd\nfrom textblob import TextBlob\n\ndef pull_tweets (query, maxTweets = 1000, tweetsPerQuery = 100, max_id = -1, sinceId = None):\n    '''\n    Finds tweets (Comment, Date, Favorites, User) for a query string.\n    Twitter API limit per query is 100. Combines these queries. \n    '''\n    \n    # Fill with your own app details\n    API_KEY = \"wCl2jflXpyWmYDM22iKFsaiS2\"\n    API_SEC = \"f3DkCao13uCfA58bSQXahsDVNF5qzNztrgt3wB2RDDAV8zyXvT\"\n    \n    # connect to Twitter using authentication\n    auth = tweepy.AppAuthHandler(API_KEY, API_SEC)\n    # wait_on_rate_limit means that if the API limit is hit, \n    #   the pulls will wait until more calls are available\n    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n    \n    # Pull comments from Twitter\n    # See https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines\n    tweetCount = 0\n    data = pd.DataFrame() \n    \n    while tweetCount < maxTweets:\n        if (max_id <= 0):\n            new_tweets = api.search(q=query, count=tweetsPerQuery, \n                                    since_id=sinceId)\n        else:\n            new_tweets = api.search(q=query, count=tweetsPerQuery,\n                                    max_id=str(max_id - 1), \n                                    since_id=sinceId)\n        if not new_tweets:\n            print(\"No more tweets found\")\n            break\n    \n        tweetCount += len(new_tweets)\n        print(\"Downloaded {0} tweets\".format(tweetCount))\n        max_id = new_tweets[-1].id\n                    \n        ## Create a dataset from the downloaded tweets\n        new_data = pd.DataFrame([{\n                'Date': tweet.created_at,\n                'Comments': tweet.text, \n                'User': tweet.user.name, \n                'Favorites': tweet.favorite_count} \n                for tweet in new_tweets])\n          \n        data = data.append(new_data)\n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pull_tweets(\"trump\", maxTweets = 5000)\n\n# In real life you might have test data with pre-labeled sentiments. We will use a simple word net algorithm to classify for now.\ndata['Polarity'] = [TextBlob(comment).polarity for comment in data.Comments]\n\ndata.loc[data['Polarity'] > 0, 'Sentiment'] = 'positive'\ndata.loc[data['Polarity'] < 0, 'Sentiment'] = 'negative'\ndata.loc[data['Polarity'] == 0, 'Sentiment'] = 'neutral'\n\n#convert data to a csv\ndata.to_csv(\"juegosPanamericanos.csv\", index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data.head(50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}