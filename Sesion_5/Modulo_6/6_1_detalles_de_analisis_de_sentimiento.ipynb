{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Naive Bayes\n\nNaive Bayes (Bayes Ingenuo) se refiere al uso del teorema de Bayes suponiendo (ingenuamente) que nuestros \"features\" son independientes.\n\nLos clasificadores Naive Bayes son populares para categorizar texto. El objetivo de este módulo es aprender los conceptos básicos de Naive Bayes y las maneras en que Naive Bayes puede ser mejorado para realizar analisis de sentimiento. El modulo está dividido en las siguientes secciones:\n\n+ Preparar la data para analisis\n+ Introducción a Naive Bayes\n+ Mejora 1: Manejo de la negación\n+ Mejora 2: Bigramas\n+ Mejora 3: Binary count Naive Bayes\n+ Mejora 4: Manejo de texto informal\n\nEste módulo coincide con la lecture sobre mejorar Naive Bayes para análisis de sentimiento.\n\nAntes de profundizar en la sección 1 acerca de la preparación de datos, comenzaremos haciendo una descripción general de los datos que estamos utilizando. Vamos a usar la siguiente metodología:\n\n1. Primero, extrajimos los datos de Twitter relacionados a microfinanzas (microfinance) usando tweepy, que es una librería API de Twitter.\n2. Después, clasificamos los comentarios en polaridades de sentimiento para obtener los datos de entrenamiento para nuestro modelo.\n    + Normalmente, este proceso requeriría clasificar manualmente el 20% de los datos (aprox.).\n    + En nuestro ejemplo, aprovechamos un algoritmo simple de wordnet llamado TextBlob para etiquetar completamente los datos.\n3. Finalmente, revisamos y corregimos manualmente las clasificaciones.\n    + (Nota, si estás interesado en ver el código utilizado para implementar esto, revisa el archivo \"twitter_web_scraping\") "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Instalar paquetes faltantes en Azure Notebooks\n!pip install wordcloud",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: wordcloud in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (1.5.0)\nRequirement already satisfied: numpy>=1.6.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from wordcloud) (1.17.0)\nRequirement already satisfied: pillow in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from wordcloud) (4.3.0)\nRequirement already satisfied: olefile in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pillow->wordcloud) (0.44)\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport string\nimport nltk\n# si nunca lo has hecho, tienes que descargar las \"stopwords\" de nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\nimport re\nimport itertools\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom wordcloud import WordCloud\n\nfrom nltk.sentiment.util import mark_negation\nfrom nltk import word_tokenize",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#cargar datos y agregar una columna para sentimiento basado en polaridad\ndata = pd.read_csv(\"microfinance_tweets.csv\", encoding=\"ISO-8859-1\")\n\n#revisar los datos para ver si todo funcionó\ndata.head(10)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comments</th>\n      <th>Date</th>\n      <th>Favorites</th>\n      <th>User</th>\n      <th>Polarity</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RT @atmadiprayET: Here's why Janalakshmi Finan...</td>\n      <td>3/22/2018 5:40</td>\n      <td>0</td>\n      <td>Saloni Shukla</td>\n      <td>-0.100000</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RT @ecosmob: Ecosmob's #Mobility solutions for...</td>\n      <td>3/22/2018 5:36</td>\n      <td>0</td>\n      <td>Sindhav Bhageerath</td>\n      <td>-0.062500</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Project have big future! Microfinance is belie...</td>\n      <td>3/22/2018 5:27</td>\n      <td>0</td>\n      <td>Konstantin #savedroidICO</td>\n      <td>0.166667</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Online #Banking-  Yako Microfinance Bank prov...</td>\n      <td>3/22/2018 5:21</td>\n      <td>0</td>\n      <td>YakoMicrofinance</td>\n      <td>0.500000</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MICROFINANCE EVENT: 3rd BoP Global Network Sum...</td>\n      <td>3/22/2018 5:19</td>\n      <td>0</td>\n      <td>MicroCapital</td>\n      <td>0.045455</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RT @FINCA: .@rupertscofield spoke with @techwe...</td>\n      <td>3/22/2018 5:08</td>\n      <td>0</td>\n      <td>Right Angles</td>\n      <td>0.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Capacity building and knowledge management are...</td>\n      <td>3/22/2018 5:03</td>\n      <td>0</td>\n      <td>@barbara_zawedde</td>\n      <td>0.144444</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RT @oluabudu: Get a cash loan of N200,000 to N...</td>\n      <td>3/22/2018 5:00</td>\n      <td>0</td>\n      <td>Tola Abudu</td>\n      <td>0.136364</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RT @PoojaMedia: From being an influencer for a...</td>\n      <td>3/22/2018 4:59</td>\n      <td>0</td>\n      <td>idee</td>\n      <td>0.000000</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>RT @WhatWorksVAWG: Read latest blog from Erin ...</td>\n      <td>3/22/2018 4:58</td>\n      <td>0</td>\n      <td>Sofia Sprechmann</td>\n      <td>0.500000</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                            Comments            Date  \\\n0  RT @atmadiprayET: Here's why Janalakshmi Finan...  3/22/2018 5:40   \n1  RT @ecosmob: Ecosmob's #Mobility solutions for...  3/22/2018 5:36   \n2  Project have big future! Microfinance is belie...  3/22/2018 5:27   \n3  #Online #Banking-  Yako Microfinance Bank prov...  3/22/2018 5:21   \n4  MICROFINANCE EVENT: 3rd BoP Global Network Sum...  3/22/2018 5:19   \n5  RT @FINCA: .@rupertscofield spoke with @techwe...  3/22/2018 5:08   \n6  Capacity building and knowledge management are...  3/22/2018 5:03   \n7  RT @oluabudu: Get a cash loan of N200,000 to N...  3/22/2018 5:00   \n8  RT @PoojaMedia: From being an influencer for a...  3/22/2018 4:59   \n9  RT @WhatWorksVAWG: Read latest blog from Erin ...  3/22/2018 4:58   \n\n   Favorites                      User  Polarity Sentiment  \n0          0             Saloni Shukla -0.100000  negative  \n1          0        Sindhav Bhageerath -0.062500   neutral  \n2          0  Konstantin #savedroidICO  0.166667  positive  \n3          0          YakoMicrofinance  0.500000  positive  \n4          0              MicroCapital  0.045455  positive  \n5          0              Right Angles  0.000000   neutral  \n6          0          @barbara_zawedde  0.144444  positive  \n7          0                Tola Abudu  0.136364  positive  \n8          0                      idee  0.000000   neutral  \n9          0          Sofia Sprechmann  0.500000  positive  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\nEl dataset de Twitter contiene seis columnas:\n\n+ Comments: el contenido del post que menciona las microfinanzas\n+ Date: fecha en que se creó el post en Twitter\n+ Favorites: número de favoritos que tiene el post\n+ User: nombre del usuario que publicó el comentario\n+ Polarity: la polaridad, que representa el grado de positividad versus negatividad asignado por textblob\n+ Sentiment: una conversión de la polaridad en clasificaciones positivas, negativas y neutrales\n\nNotemos que este ejercicio simula lo que sucedería en una situación del mundo real. Nuestros datos son limitados, y el proceso de obtener datos de calidad para entrenamiento y test (con polaridad etiquetada) es difícil. Para comprender la naturaleza de nuestros datos, realizaremos algunos ejercicios a continuación."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Revisemos el desglose para Sentiment\nsns.countplot(x = 'Sentiment', data = data)\ndata['Sentiment'].value_counts()",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "neutral     1943\npositive    1039\nnegative     259\nName: Sentiment, dtype: int64"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f75f13c1cc0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Parece que la mayoría de las personas tienen comentarios positivos o neutrales sobre microfinanzas. De hecho, el volumen de comentarios negativos es el más interesante y debe investigarse más a fondo."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## ¿Cuáles son estos comentarios negativos?\n\n#formatear columnas para que los comentarios no se corten\npd.set_option('display.max_columns', None) \n\n#extraer comentarios negativos\ndata.loc[data['Sentiment'] == 'negative']['Comments'].unique()[0:10]",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "array([\"RT @atmadiprayET: Here's why Janalakshmi Financial takes so long to small finance bank transition https://t.co/traKQNXuxe via @economictime\\x85\",\n       \"Here's why Janalakshmi Financial takes so long to small finance bank transition https://t.co/traKQNXuxe via\\x85 https://t.co/6fk3hc1S9I\",\n       'RT @The_LHoFT: Why Luxembourg? ???\\nA few of the reasons why #Luxembourg is home to a growing number of financial services innovators, inclu\\x85',\n       '@DataMufti Isnt it surprising that Telenor sold half of its Tameer microfinance stake to Alipay!!! To me it looks like a wrong move.',\n       '+???? ????? ???? ??????? ???? \\n?????? :NMB MICROFINANCE BITTIYA SANSTHA LIMITED \\n?? \\n?.Junior Field Officer \\n?.Assi\\x85 https://t.co/4onBO7RMTc',\n       'HR Administrative Assistant at Crossover Microfinance Bank Limited Crossover Microfinance Bank Limited, is ... https://t.co/C9b5PAKKm9',\n       'If the bank of Ghana is being this harsh on commercial banks, what will become of savings and loans and microfinanc\\x85 https://t.co/pNO7yTJPS8',\n       'RT @ankuranga: NPLs ratio in the Microfinance Sector decreased from 9.0% in December 2016 to 8.2% in December 2017 \\n\\n#MPFSS2018 #Rwanda #Rw\\x85',\n       'Valued customer,\\nBeware of this Fake page impersonating Faulu Kenya. Faulu Microfinance  Bank does not have any con\\x85 https://t.co/aGZlMAZqag',\n       '#Salary #Loan - Yako Microfinance #Salary Loan to Employees of reputed organisations having at least six months of\\x85 https://t.co/SVKpFmDlfu'],\n      dtype=object)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Parece que algunos de los comentarios negativos no son válidos y en realidad deberían clasificarse como neutrales. Sin embargo, algunos de los comentarios negativos son válidos, por ejemplo actividad ilegal.\n\nPor lo general, limpiaríamos estas clasificaciones para corregirlo, pero avanzaremos para preparar nuestros datos para el modelo."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Preparar datos para el modelo\n\nLa forma más común de aplicar algoritmos de aprendizaje automático, como Naive Bayes, al análisis de texto es convertir nuestro texto en vectores de características numéricas que los algoritmos puedan entender.\n\nLa representación que crearemos es la **Bag of Words**. Para pasar de nuestros datos, que  están estructurados como una serie de strings, a una bolsa de palabras, **vectorizaremos** nuestros tweets.\n\nEsta **vectorización** de nuestros documentos de texto requerirá algunos pasos clave, que describimos con más detalle en el [Módulo 8]:https://docs.google.com/presentation/d/1vaxDuUROgaqix9Mfkyb4_h0cGkGwN7AUeLU1ozhtJW8/edit#slide=id.g227403103b_0_1171\n\nPaso 0. Divide los datos en conjuntos de entrenamiento y prueba (esto no es parte de la vectorización) <br>\nPaso 1. Limpia y **tokeniza** el texto <br>\nPaso 2. Cuenta la cantidad de palabras en cada documento\n\n#### Paso 0: Crear datos de entrenamiento y prueba\nEste paso debería venir naturalmente.\nHemos clasificado los sentimientos para todos nuestros datos, y elegimos una división de entrenamiento y prueba en proporción 80-20, con observaciones asignadas al azar."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# A) Creamos un conjunto de datos de prueba (20% de datos) y de entrenamiento (80% de datos)\n# Establecemos una semilla aleatoria para que pueda ser reproducible\n\nnp.random.seed(12345)\nmsk = np.random.rand(len(data)) < 0.8\ntrain = data[msk]\ntest = data[~msk]",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Verificamos el tamaño de los conjuntos de entrenamiento y prueba para confirmar que los datos\n# se dividieron en las proporciones correctas\nprint(len(train), len(test))\nprint(\"Split of {:.2%} train, {:.2%} test\".format(len(train)/len(data), len(test)/len(data)))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2594 647\nSplit of 80.04% train, 19.96% test\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Paso 1: Limpiar y Tokenizar texto\n\nComo se mencionó, la forma más común de extraer vectores numéricos del texto es tener un conjunto de **tokens** que podemos contar.\n\n**Tokenización** significa convertir un documento / oración / tweet en *tokens* individuales, que generalmente son palabras o unidades similares a palabras.\n\nAl crear estos tokens similares a palabras, queremos estandarizarlos de manera que agrupen palabras con una intención similar. Esto hace que estas características de token sean más útiles que las palabras no procesadas. ¡Mira el [Módulo NLP] (https://docs.google.com/presentation/d/1vaxDuUROgaqix9Mfkyb4_h0cGkGwN7AUeLU1ozhtJW8/edit#slide=id.g227403103b_0_1050) para más detalles!\n\nCreamos una función para\n+ **lematizar** estas palabras\n+ eliminar *stop words*, puntuación, URL, números y caracteres especiales\n\nAfortunadamente, NLTK tiene diferentes bibliotecas que hacen que esta tarea sea muy fácil."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# B) Lematizar palabras y eliminar stop words y puntuaciones\nlemmatizer = WordNetLemmatizer()\n\ndef text_process(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove URLs\n    2. Remove punctuation\n    3. Remove numbers\n    4. Remove stopwords\n    5. Return the cleaned text as a list of lemmatized words\n    '''\n    \n    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text, flags=re.MULTILINE)\n    \n    text = ''.join([t for t in text if t not in string.punctuation])\n    \n    text = ''.join([t for t in text if not t.isdigit()])\n    \n    return [lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stopwords.words('english')]",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Probamos la función para asegurarnos que todo funciona tal como lo esperamos\nsample_text = \"I LOVE love BBQ chicken pizza. I eat it every day, but sometimes I wonder if I'm being unhealthy https://ilove.pizza\"\ntext_process(sample_text)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "['LOVE',\n 'love',\n 'BBQ',\n 'chicken',\n 'pizza',\n 'eat',\n 'every',\n 'day',\n 'sometimes',\n 'wonder',\n 'Im',\n 'unhealthy']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Paso 2) Contar el número de palabras en cada 'documento'\n\nEste es el paso que lleva nuestro texto a una representación de *Bag of Words*.\n\nUna bolsa de palabras o *Bag of Words* nos permite trabajar con texto de forma estructurada. Utilizamos un recuento simple de palabras, aunque las bibliotecas de sklearn TF-IDF también están disponibles y pueden usarse para otros problemas de NLP.\n\nUsamos la función **CountVectorizer** de la biblioteca sklearn.\n\nEl vectorizador TF-IDF está disponible usando:\n\n`from sklearn.feature_extraction.text import TfidfVectorizer`\n\nNota la diferencia en el uso de las funciones para ajustar la *bag of words* para datos de prueba versus entrenamiento."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# C) Representar el texto como bag-of-words aprovechando el CountVectorizer de sklearn\nvectorizer = CountVectorizer(analyzer=text_process)\ntrain_features = vectorizer.fit_transform(train['Comments'])\ntest_features =  vectorizer.transform(test['Comments'])",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vectorizer.get_feature_names()[0:20]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Estas son nuestras *características* o *features*, es decir, todos los tokens limpios de todos los tweets. Un tweet de entrenamiento o prueba ahora se puede representar como un recuento de cuántas veces aparece cualquiera de estas características."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Probar el vectorizador revisando el tamaño de los vectores\nprint('Length of Vectorizer Vocabulary: ', len(vectorizer.vocabulary_))\nprint('Shape of Sparse Matrix: ', test_features.shape)\nprint('Amount of Non-Zero occurrences: ', test_features.nnz)\n\n# Percentage of non-zero values\ndensity = 100.0 * (test_features.nnz / (test_features.shape[0] * test_features.shape[1]))\nprint('Density: {}'.format((density)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tenemos 4890 *tokens* en nuestro corpus. La matriz de características de datos de prueba es una representación de cada uno de los 647 tweets como un vector de recuento de tokens."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Para entender cómo el vectorizer guarda las palabras, revisa el efecto de la función en un Tweet\nsample_test = test['Comments'].iloc[0]\nprint(\"Sample comment: \", sample_test, \"\\n\")\nsample_vector = vectorizer.transform([sample_test])\nprint(\"Vectorization:\")\nprint(sample_vector)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Esto significa que tenemos 1 ocurrencia para una palabra en el indice 755, 925, etc.\n# Podemos usar la función get_feature_names() del vectorizer para ver las palabras correspondientes\nprint(list(vectorizer.get_feature_names()[i] for i in sample_vector.indices))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Veamos una nube de palabras o *wordclod* de nuestros datos para ver cuales son las palabras que aparecen con mayor frecuencia!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cloud = WordCloud(width=1600, height=1200, collocations=False).generate(\" \".join(data['Comments'].astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Todavía parece que tenemos algunas URL, y Telenor parece ser un gran problema."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introducción a Naive Bayes\nComenzaremos ejecutando Naive Bayes en el texto y comprobaremos qué tan bien funciona en la predicción de clasificaciones. Este paso es muy fácil dado que ya hemos preparado los datos en un formato de bolsa de palabras. <br>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Entrenamos usando multinomial NB de sklearn\nnb = MultinomialNB()\nnb.fit(train_features, train['Sentiment'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Test\npreds = nb.predict(test_features)\n\n#imprimir la precisión del modelo\naccuracy = (preds == test['Sentiment'])\n'Accuracy : {:.2%}'.format(accuracy.sum() / len(accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "La precisión es alta, ~ 84%. La mayoría de los algoritmos de análisis de sentimientos tienen una precisión de ~ 80% cuando ha ajustado el modelo en los datos de entrenamiento adecuados. En este caso, la precisión del 84% probablemente se infla porque hemos usado TextBlob para determinar el sentimiento correcto en nuestros datos de entrenamiento.\n\nLa medida actual de precisión es un valor unidimensional. No sabemos en qué nos estamos equivocando cuando clasificamos mal los sentimientos. Una herramienta para comprender mejor cómo estamos clasificando erróneamente se llama matriz de confusión (introducida por primera vez en el módulo X: Regresión logística)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class_names = set(data['Sentiment'])\ncnf_matrix = confusion_matrix(test['Sentiment'], preds)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, basic NB')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Los resultados anteriores muestran que comúnmente estamos prediciendo comentarios positivos como neturales. Esta clasificación errónea no es tan preocupante como nuestra segunda clasificación errónea común de comentarios negativos como positivos."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Mejora 1: Manejo de la negación\n\nPara mejorar Naive Bayes para el análisis de sentimientos, podemos ajustar nuestro tokenizador para la negación en las oraciones. (Consulta las diapositivas de la lección del módulo 10 para obtener más detalles). *Mark negation* es una librería que podemos usar en NLTK que agrega un \\\"_NEG\\\" a las palabras después de una negación hasta un signo de puntuación. Esto significa que palabras después de una negación se manejarán de manera diferente a las palabras anteriores a una negación. Podemos investigar si el uso de la negación mejorará nuestro modelo. Dado que Wordnet no clasificó correctamente la negación, este método probablemente no mejorará el modelo."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Ejemplo de marcado de negación\nprint(mark_negation(\"I like BBQ chicken pizza . I like pepperoni pizza.\".split()))\nprint(mark_negation(\"I don't like BBQ chicken pizza . I like pepperoni pizza only .\".split()))\n\n# Test usando mark_negation con text_process\ntext = \"I don't like BBQ chicken pizza. I like pepperoni pizza only.\"\nprint(mark_negation(text_process(text)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Para incorporarlo al modelo Naive Bayes, necesitamos ajustar como procesamos la data para crear el bag of words\nvectorizer_negation = CountVectorizer(analyzer='word', tokenizer = lambda text: mark_negation(text_process(text)))\ntrain_features_negation = vectorizer_negation.fit_transform(train['Comments'])\ntest_features_negation = vectorizer_negation.transform(test['Comments'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#entrenar un multinomial NB de sklearn\nnb = MultinomialNB()\nnb.fit(train_features_negation, train['Sentiment'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Test\npreds = nb.predict(test_features_negation)\n\n#imprimir la precisión del modelo\naccuracy = (preds == test['Sentiment'])\n'Accuracy : {:.2%}'.format(accuracy.sum() / len(accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Como esperabamos, usar la negación no mejoró la precisión. De hecho, la empeoró!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Mejora 2: Bigrams\n¿Qué pasa si intentamos usar bigramas? ¿Cómo cambiaría la precisión de nuestro modelo el uso de bigramas? Por lo general, incluir bigramas es un buen refuerzo para cualquier tipo de modelo Naive Bayes, no solo aplicado a análisis de sentimientos. CountVectorizer nos facilita la entrada de bigramas en nuestro modelo Naive Bayes usando el parámetro ngram_range. El primer valor en la tupla especifica el ngrama más bajo que rastrearemos, y el segundo, el más alto. En este caso, consideraremos tanto unigramas como bigramas. <br>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Usar bigramas con countvectorizer\nvectorizer_bigrams = CountVectorizer(analyzer=\"word\", ngram_range = (1,2), encoding='ISO-8859-1', lowercase=False)\ntrain_features_bigrams = vectorizer_bigrams.fit_transform(train['Comments'])\ntest_features_bigrams  = vectorizer_bigrams.transform(test['Comments'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Testear la función\nvectorizer_bigrams.vocabulary_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Entrenar un multinomial NB de sklearn\nnb = MultinomialNB()\nnb.fit(train_features_bigrams, train['Sentiment'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Test\npreds = nb.predict(test_features_bigrams)\n\n#imprimir la precisión del modelo\naccuracy = (preds == test['Sentiment'])\n'Accuracy : {:.2%}'.format(accuracy.sum() / len(accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Como era de esperar, agregar bigramas mejora ligeramente la precisión de nuestro modelo Naive Bayes. <br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Mejora 3: recuento binario Naive Bayes\n\nOtra mejora específica para el análisis de sentimientos es el conteo binario. El uso del conteo binario con Naive Bayes no garantiza que mejore el modelo, pero con frecuencia este lo hace cuando se trata de análisis de sentimientos. Para incorporar esto a nuestro modelo, actualizaremos la función text_process para eliminar duplicados."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#actualizar la función text_process para eliminar duplicados\n\ndef text_process_binary(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Return the cleaned lemmatized text as a list of words\n    '''\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    \n    nostopwords = [lemmatizer.lemmatize(word) for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    return list(set(nostopwords))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Usar bigramas con countvectorizer\nvectorizer_binary = CountVectorizer(analyzer=text_process_binary)\ntrain_features_binary = vectorizer_binary.fit_transform(train['Comments'])\ntest_features_binary =  vectorizer_binary.transform(test['Comments'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Entrenar un multinomial NB de sklearn\nnb = MultinomialNB()\nnb.fit(train_features_binary, train['Sentiment'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#test\npreds = nb.predict(test_features_binary)\n\n#imprimir la precisión del modelo\naccuracy = (preds == test['Sentiment'])\n'Accuracy : {:.2%}'.format(accuracy.sum() / len(accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\nDesafortunadamente, agregar un conteo binario no mejora la precisión de nuestro modelo Naive Bayes en esta muestra de datos. <br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Mejora 4: Tratar con texto informal\nPor último, otra mejora específica para el análisis de sentimientos es tratar más específicamente con texto informal. Los textos informales comunmente relacionados con el análisis de sentimientos son emojis y textos informales, como texto totalmente en mayúsculas, para representar la emoción. Para tratar con emojis, actualizaremos nuestra función text_process, y para tratar con mayúsculas, desactivaremos el parámetro en minúsculas en el inicializador CountVectorizer. Dado que wordnet no consideró las mayúsculas y los emojis al clasificar los datos en primer lugar, desafortunadamente este método probablemente no cambiará la precisión del modelo."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#el texto en mayúscula se considera una palabra diferente, y emojis simples\n\ndef text_process_informal(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Return the cleaned text as a list of lemmatized words\n    '''\n    nopunc = [text[i] for i in range(len(text)) if (text[i] not in string.punctuation) or \\\n              (text[i:i+2] in [\":(\", \":)\"]) or (text[i]+text[i-1] in [\":(\", \":)\"]) or \\\n              (text[i:i+2] in [\"(:\", \"):\"]) or (text[i]+text[i-1] in [\"(:\", \"):\"])]\n    nopunc = ''.join(nopunc)\n    \n    nostopwords = [lemmatizer.lemmatize(word) for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    return nostopwords",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#test para asegurarse que la función funciona bien\ntext = \":I LOVE love BBQ chicken pizza. I eat it every day, but sometimes I wonder if I'm being unhealthy :) :(\"\ntext_process_informal(text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Usar bigramas con countvectorizer\nvectorizer_informal = CountVectorizer(analyzer=text_process_informal, lowercase = False)\ntrain_features_informal = vectorizer_informal.fit_transform(train['Comments'])\ntest_features_informal =  vectorizer_informal.transform(test['Comments'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Entrenar un multinomial NB de sklearn\nnb = MultinomialNB()\nnb.fit(train_features_informal, train['Sentiment'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#test\npreds = nb.predict(test_features_informal)\n\n#imprimir la precisión del modelo\naccuracy = (preds == test['Sentiment'])\n'Accuracy : {:.2%}'.format(accuracy.sum() / len(accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Como se predijo, la precisión no cambió de Naive Bayes básico."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Conclusión\n\nHemos revisado los conceptos básicos de Naive Bayes e investigado cómo se pueden usar las mejoras en el caso del análisis de sentimientos. Nuestros resultados están un poco sesgados debido a la metodología de clasificación original, pero parece que usar bigramas (o cualquier forma de ngramas) fue la forma más efectiva de mejorar el modelo.\n\nEsperamos que este módulo te haya enseñado que los datos de la vida real son desordenados y que construir un modelo es más un arte que una ciencia. Hay muchos tipos de pequeñas mejoras que puede hacer en un modelo Naive Bayes, y a menudo depende de la naturaleza de los datos. Por ejemplo, si sabes que el texto que estás utilizando será particularmente informal, puede ser bueno adaptar la función de tu analizador para tener en cuenta esas informalidades. Si el texto no es informal, este ajuste no cambiará mucho a la precisión del modelo."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Sources: https://medium.com/tensorist/classifying-yelp-reviews-using-nltk-and-scikit-learn-c58e71e962d9, https://www.dataquest.io/blog/naive-bayes-tutorial/, http://nlpforhackers.io/sentiment-analysis-intro/, http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html <br>"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}